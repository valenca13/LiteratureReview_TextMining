# Select only the abstract
A06_10 <- A06_10$Abstract
#Data Cleaning
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A06_10)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A06_10 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
#Consider two similar words as the same.
A06_10 <- tm_map(A06_10, PlainTextDocument)
strings <- c("cities")
abstracts <- str_replace_all(A06_10, strings, "city")
A06_10final <- data.frame(abstracts)
#Tokenize words from abstracts
tokenizing_abstract <- A06_10final %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstracts) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
adicional_stopwords <- tribble(
~word,~lexicon,
"study", "CUSTOM",
"paper", "CUSTOM",
"article", "CUSTOM",
"based", "CUSTOM",
"results", "CUSTOM",
"processes", "CUSTOM",
"research", "CUSTOM",
"datum", "CUSTOM")
stop_words2 <- stop_words %>%
bind_rows(adicional_stopwords)
#Add new stopwords and repeat the tokenization process
tokenizing_abstract <- A06_10final %>%
mutate(id = row_number()) %>%
unnest_tokens(word, abstracts) %>%
anti_join(stop_words2)
#Frequency of words
word_counts <- tokenizing_abstract %>%
count(word) %>%
filter(n>10) %>% ##Filter words that appear more than "n" times
mutate(word2 = fct_reorder(word, n)) %>%
arrange(desc(n))
#Plot word count
ggplot(
word_counts, aes (x = word2, y = n/max(n), fill = "red")
) +
geom_col() +
coord_flip() +
labs(
title = "Most frequent words of filtered abstracts: 2006 - 2010",
x = "Words", y ="Normalized frequency - Total of 31 papers"
)
# Select papers from 2011 - 2015
A11_15 <- df[df$Year >= 2011 & df$Year<= 2015,]
# Select only the abstract
A11_15 <- A11_15$Abstract
#Data Cleaning
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A11_15)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A11_15 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
#Consider two similar words as the same
A11_15 <- tm_map(A11_15, PlainTextDocument)
strings <- c("cities")
abstracts <- str_replace_all(A11_15, strings, "city")
A11_15final <- data.frame(abstracts)
#Tokenize words from abstracts
tokenizing_abstract <- A11_15final %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstracts) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
adicional_stopwords <- tribble(
~word,~lexicon,
"study", "CUSTOM",
"paper", "CUSTOM",
"article", "CUSTOM",
"based", "CUSTOM",
"results", "CUSTOM",
"research", "CUSTOM")
stop_words2 <- stop_words %>%
bind_rows(adicional_stopwords)
#Add new stopwords and repeat the tokenization process
tokenizing_abstract <- A11_15final %>%
mutate(id = row_number()) %>%
unnest_tokens(word, abstracts) %>%
anti_join(stop_words2)
#Frequency of words
word_counts <- tokenizing_abstract %>%
count(word) %>%
filter(n>40) %>% #Filtrar palavras que aparecem apenas mais de 100 vezes
mutate(word2 = fct_reorder(word, n)) %>% #Ordenar no gr?fico por ordem decrescente
arrange(desc(n)) #Colocar em ordem decrescente nas contagens
#Plot word count
ggplot(
word_counts, aes (x = word2, y = n/216, fill = "red")
) +
geom_col() + #Criar gr?fico
coord_flip() + #inverter gr?fico pra mostrar as palavras
labs(
title = "Most frequent words of filtered abstracts: 2011 - 2015",
x = "Words", y ="Normalized frequency - Total of 103 papers"
)
# Select papers from 2011 - 2015
A11_15 <- df[df$Year >= 2011 & df$Year<= 2015,]
# Select only the abstract
A11_15 <- A11_15$Abstract
#Data Cleaning
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A11_15)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A11_15 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
#Consider two similar words as the same
A11_15 <- tm_map(A11_15, PlainTextDocument)
strings <- c("cities")
abstracts <- str_replace_all(A11_15, strings, "city")
A11_15final <- data.frame(abstracts)
#Tokenize words from abstracts
tokenizing_abstract <- A11_15final %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstracts) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
adicional_stopwords <- tribble(
~word,~lexicon,
"study", "CUSTOM",
"paper", "CUSTOM",
"article", "CUSTOM",
"based", "CUSTOM",
"results", "CUSTOM",
"research", "CUSTOM",
"datum", "CUSTOM")
stop_words2 <- stop_words %>%
bind_rows(adicional_stopwords)
#Add new stopwords and repeat the tokenization process
tokenizing_abstract <- A11_15final %>%
mutate(id = row_number()) %>%
unnest_tokens(word, abstracts) %>%
anti_join(stop_words2)
#Frequency of words
word_counts <- tokenizing_abstract %>%
count(word) %>%
filter(n>40) %>% #Filtrar palavras que aparecem apenas mais de 100 vezes
mutate(word2 = fct_reorder(word, n)) %>% #Ordenar no gr?fico por ordem decrescente
arrange(desc(n)) #Colocar em ordem decrescente nas contagens
#Plot word count
ggplot(
word_counts, aes (x = word2, y = n/216, fill = "red")
) +
geom_col() + #Criar gr?fico
coord_flip() + #inverter gr?fico pra mostrar as palavras
labs(
title = "Most frequent words of filtered abstracts: 2011 - 2015",
x = "Words", y ="Normalized frequency - Total of 103 papers"
)
# Select papers from 2016 - 2020
A16_20 <- df[df$Year >= 2016 & df$Year<=2020,]
# Select only the abstract
A16_20 <- A16_20$Abstract
#Data Cleaning
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A16_20)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A16_20 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
#Consider two similar words as the same
A16_20 <- tm_map(A16_20, PlainTextDocument)
strings <- c("cities")
abstracts <- str_replace_all(A16_20, strings, "city")
A16_20final <- data.frame(abstracts)
#Tokenize words from abstracts
tokenizing_abstract <- A16_20final %>%
mutate(id = row_number()) %>% #Criar novo ID para cada paper
unnest_tokens(word, abstracts) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
adicional_stopwords <- tribble(
~word,~lexicon,
"study", "CUSTOM",
"paper", "CUSTOM",
"article", "CUSTOM",
"based", "CUSTOM",
"information","CUSTOM",
"analysis", "CUSTOM",
"discussion","CUSTOM",
"results", "CUSTOM",
"research", "CUSTOM")
adicional_stopwords <- tribble(
~word,~lexicon,
"study", "CUSTOM",
"paper", "CUSTOM",
"article", "CUSTOM",
"based", "CUSTOM",
"information","CUSTOM",
"analysis", "CUSTOM",
"discussion","CUSTOM",
"results", "CUSTOM",
"research", "CUSTOM",
"datum", "CUSTOM")
stop_words2 <- stop_words %>%
bind_rows(adicional_stopwords)
#Add new stopwords and repeat the tokenization process
tokenizing_abstract <- A16_20final %>%
mutate(id = row_number()) %>% #Criar novo ID para cada paper
unnest_tokens(word, abstracts) %>%
anti_join(stop_words2) #Remove stopwords (english package of stopwords)
#Frequency of words
word_counts <- tokenizing_abstract %>%
count(word) %>%
filter(n>130) %>% #Filtrar palavras que aparecem apenas mais de 100 vezes
mutate(word2 = fct_reorder(word, n)) %>% #Ordenar no gr?fico por ordem decrescente
arrange(desc(n)) #Colocar em ordem decrescente nas contagens
#Plot word count
ggplot(
word_counts, aes (x = word2, y = n/max(n), fill = "red")
) +
geom_col() +
coord_flip() +
labs(
title = "Most frequent words of filtered abstracts: 2016 - 2020",
x = "Words", y ="Normalized frequency - Total of 326 papers"
)
knitr::opts_chunk$set(echo = TRUE)
df <- data.frame(systematicreview)
summary(df)
df <- data.frame(systematicreview)
view(df)
tokenizing_abstract <- A78_85final %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstracts) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
View(tokenizing_abstract)
library(tm)
library(NLP)
library(tidyverse)
library(tidytext)
library(readxl)
library(textstem)
systematicreview <- read_excel("Data/table_systematic_review.xlsx")
df <- data.frame(systematicreview)
#Remove column (authors)
df <- df[,-c(7)]
# Filter only papers that were considered relevant for the analysis. "Frequent abstract" = 1.
df <- subset(df, df[2]=="1")
# Select papers from 1978 - 1985
A78_85 <- df[df$Year >= 1978 & df$Year<=1985,]
# Select only the abstract
A78_85 <- A78_85$Abstract
#Data Cleaning
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
#Tokenize words from abstracts
tokenizing_abstract <- A78_85 %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstracts) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
# Consider two similar words as the same.
A78_85 <- tm_map(A78_85, PlainTextDocument)
#strings <- c("neighborhoods")
#abstracts <- str_replace_all(A78_85, strings, "neighborhood")
A78_85final <- data.frame(A78_85)
#Tokenize words from abstracts
tokenizing_abstract <- A78_85final %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstracts) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
View(A78_85final)
library(tm)
library(NLP)
library(tidyverse)
library(tidytext)
library(readxl)
library(textstem)
systematicreview <- read_excel("Data/table_systematic_review.xlsx")
df <- data.frame(systematicreview)
df <- df[,-c(7)]
df <- subset(df, df[2]=="1")
A78_85 <- df[df$Year >= 1978 & df$Year<=1985,]
A78_85 <- A78_85$Abstract
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
A78_85 <- tm_map(A78_85, PlainTextDocument)
library(tm)
library(NLP)
library(tidyverse)
library(tidytext)
library(readxl)
library(textstem)
systematicreview <- read_excel("Data/table_systematic_review.xlsx")
df <- data.frame(systematicreview)
#Remove column (authors)
df <- df[,-c(7)]
# Filter only papers that were considered relevant for the analysis. "Frequent abstract" = 1.
df <- subset(df, df[2]=="1")
# Select papers from 1978 - 1985
A78_85 <- df[df$Year >= 1978 & df$Year<=1985,]
# Select only the abstract
A78_85 <- A78_85$Abstract
#Data Cleaning
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
# Consider two similar words as the same. In this case we pted not to use this funciton.
A78_85 <- tm_map(A78_85, PlainTextDocument)
strings <- c("x")
abstracts <- str_replace_all(A78_85, strings, "x")
df <- df[,-c(7)]
df <- subset(df, df[2]=="1")
A78_85 <- df[df$Year >= 1978 & df$Year<=1985,]
A78_85 <- A78_85$Abstract
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
#A78_85 <- tm_map(A78_85, PlainTextDocument)
#strings <- c("x")
#abstracts <- str_replace_all(A78_85, strings, "x")
A78_85final <- data.frame(A78_85)
tokenizing_abstract <- A78_85final %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
View(A78_85final)
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
A78_85 <- tm_map(text3, PlainTextDocument)
systematicreview <- read_excel("Data/table_systematic_review.xlsx")
df <- data.frame(systematicreview)
df <- df[,-c(7)]
df <- subset(df, df[2]=="1")
A78_85 <- df[df$Year >= 1978 & df$Year<=1985,]
View(A78_85)
A78_85 <- A78_85$Abstract
A78_85 <- A78_85$Abstract
A78_85 <- data.frame(A78_85$Abstract)
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
corpus <- Corpus(VectorSource(A78_85))
View(corpus)
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
corpus <- Corpus(VectorSource(A78_85))
dtm <- DocumentTermMatrix(corpus)
View(dtm)
View(dtm)
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
corpus <- Corpus(VectorSource(A78_85))
dtm <- DocumentTermMatrix(corpus)
dtm.matrix <- as.matrix(dtm)
wordcount <- colSums(dtm.matrix)
systematicreview <- read_excel("Data/table_systematic_review.xlsx")
df <- data.frame(systematicreview)
df <- df[,-c(7)]
df <- subset(df, df[2]=="1")
A78_85 <- df[df$Year >= 1978 & df$Year<=1985,]
View(A78_85)
A78_85 <- df[df$Year >= 1978 & df$Year<=1985 & df$Abstract,]
View(A78_85)
A78_85 <- A78_85[,8]
systematicreview <- read_excel("Data/table_systematic_review.xlsx")
df <- data.frame(systematicreview)
df <- df[,-c(7)]
df <- subset(df, df[2]=="1")
A78_85 <- df[df$Year >= 1978 & df$Year<=1985,]
View(A78_85)
library(tm)
library(NLP)
library(tidyverse)
library(tidytext)
library(readxl)
library(textstem)
systematicreview <- read_excel("Data/table_systematic_review.xlsx")
df <- data.frame(systematicreview)
df <- df[,-c(7)]
df <- subset(df, df[2]=="1")
A78_85 <- df[df$Year >= 1978 & df$Year<=1985,]
A78_85 <- A78_85[,-c(1,2,3,4,5,6,7)]
A78_85 <- df[df$Year >= 1978 & df$Year<=1985,]
A78_85 <- data.frame(A78_85[,-c(1,2,3,4,5,6,7)])
View(A78_85)
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
A78_85 <- tm_map(A78_85, PlainTextDocument)
A78_85 <- df[df$Year >= 1978 & df$Year<=1985,]
A78_85 <- data.frame(A78_85[,-c(1,2,3,4,5,6,7)])
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
A78_85 <- Corpus(A78_85)
A78_85 <- A78_85$Abstract
df <- df[,-c(7)]
df <- subset(df, df[2]=="1")
A78_85 <- df[df$Year >= 1978 & df$Year<=1985,]
A78_85 <- A78_85$Abstract
A78_85 <- tm_map(A78_85, PlainTextDocument)
