ldaOut <- LDA(dtm,
k,
method="Gibbs",
control=list(seed = 42))
lda_topics <- ldaOut %>%
tidy(matrix = "beta") %>%
arrange(-beta)
# select 15 most frequent terms in each topic
word_probs <- lda_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
#Create term2, a factor ordered by word probability
mutate(term = fct_reorder(term, beta))
# Plot term2 and the word probabilities
ggplot(
word_probs,
aes(term,beta,fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
# Facet the bar plot by topic
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(x = "term")
# select 15 most frequent terms in each topic
word_probs <- lda_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Plot term2 and the word probabilities
ggplot(
word_probs,
aes(term,beta,fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
# Facet the bar plot by topic
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(x = "term")
# select 15 most frequent terms in each topic
word_probs <- lda_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(term, -beta)
# Plot term2 and the word probabilities
ggplot(
word_probs,
aes(term,beta,fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
# Facet the bar plot by topic
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(x = "term")
# select 15 most frequent terms in each topic
word_probs <- lda_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
#Create term2, a factor ordered by word probability
mutate(term2 = fct_reorder(term, beta))
# Plot term2 and the word probabilities
ggplot(
word_probs,
aes(term2,beta,fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
# Facet the bar plot by topic
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(x = "term")
#Run LDA using Gibbs sampling
ldaOut <- LDA(dtm,
k,
method="Gibbs",
control=list(seed = 42))
lda_topics <- ldaOut %>%
tidy(matrix = "beta") %>%
arrange(topic, -beta)
# select 15 most frequent terms in each topic
word_probs <- lda_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
#Create term2, a factor ordered by word probability
mutate(term2 = fct_reorder(term, beta))
# Plot term2 and the word probabilities
ggplot(
word_probs,
aes(term2,beta,fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
# Facet the bar plot by topic
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(x = "term")
lda_topics <- ldaOut %>%
tidy(matrix = "beta")
# select 15 most frequent terms in each topic
word_probs <- lda_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
#Create term2, a factor ordered by word probability
mutate(term2 = fct_reorder(term, beta))
# Plot term2 and the word probabilities
ggplot(
word_probs,
aes(term2,beta,fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
# Facet the bar plot by topic
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(x = "term")
knitr::opts_chunk$set(echo = TRUE)
folder <- "Data\\Full papers for LDA_bigrams"
filelist <- list.files(folder, pattern = ".txt") #select only documents ".txt"
filelist <- paste(folder, "\\", filelist, sep="") #Join documents.
x <- lapply(filelist, FUN = readLines) #Considers each line as a different element (document).
docs <- lapply(x, FUN = paste, collapse = " ")
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", docs)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
text6 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
adicional_stopwords <- c("good","represent", "present", "different","london", "may","datum","taipei", "numb", "much", "one", "two", "can", "fig", "will", "arm", "along", "xpj", "figure", "thus","aviv", "tel", "dsc","dscs","traf","also","study", stopwords("en"))
#remove stopwords
text7 <- removeWords(text6, adicional_stopwords)
# Remove words for bigrams
new_stopwords <- c("ow", "ï", "exible", "cantly","wick", "â", "exibility", "uence", "uences", "ned")
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", docs)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
text6 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
adicional_stopwords <- c("good","represent", "present", "different","london", "may","datum","taipei", "numb", "much", "one", "two", "can", "fig", "will", "arm", "along", "xpj", "figure", "thus","aviv", "tel", "dsc","dscs","traf","also","study", stopwords("en"))
#remove stopwords
text7 <- removeWords(text6, adicional_stopwords)
# Remove words for bigrams
new_stopwords <- c("ow", "exible", "cantly","wick", "exibility", "uence", "uences", "ned")
text_bigram <- removeWords(text7, new_stopwords)
corpus <- Corpus(VectorSource(text7))
dtm <- DocumentTermMatrix(corpus)
str(dtm)
dtm.matrix <- as.matrix(dtm)
wordcount <- colSums(dtm.matrix)
topten <- head(sort(wordcount, decreasing=TRUE), 10)
print(topten)
k <- 6
ldaOut <- LDA(dtm,
k,
method="Gibbs",
control=list(seed = 42))
lda_topics <- ldaOut %>%
tidy(matrix = "beta") %>%
arrange(desc(beta))
# select 15 most frequent terms in each topic
word_probs <- lda_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
#Create term2, a factor ordered by word probability
mutate(term2 = fct_reorder(term, beta))
# Plot term2 and the word probabilities
ggplot(
word_probs,
aes(term2,beta,fill = as.factor(topic))
) + geom_col(show.legend = FALSE) +
# Facet the bar plot by topic
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(x = "term")
lda_topics <- LDA(corpus,
k,
method="Gibbs",
control=list(seed = 42))
lda_topics <- ldaOut %>%
tidy(matrix = "beta")
View(lda_topics)
View(ldaOut)
#Run LDA using Gibbs sampling
ldaOut <- LDA(dtm,
k,
method="Gibbs",
control=list(seed = 42))
lda_topics <- ldaOut %>%
tidy(matrix = "beta") %>%
arrange(desc(beta))
# select 15 most frequent terms in each topic
word_probs <- lda_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
#Create term2, a factor ordered by word probability
mutate(term2 = fct_reorder(term, beta))
# Plot term2 and the word probabilities
ggplot(
word_probs,
aes(term2,beta,fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
# Facet the bar plot by topic
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(x = "term")
View(lda_topics)
View(word_probs)
# select 15 most frequent terms in each topic
word_probs <- lda_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() #%>%
# Plot term2 and the word probabilities
ggplot(
word_probs,
aes(term2,beta,fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
# Facet the bar plot by topic
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(x = "term")
# Plot term2 and the word probabilities
ggplot(
word_probs,
aes(term,beta,fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
# Facet the bar plot by topic
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(x = "term")
# select 15 most frequent terms in each topic
word_probs <- lda_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() #%>%
#Create term2, a factor ordered by word probability
mutate(term2 = fct_reorder(term, beta))
# select 15 most frequent terms in each topic
word_probs <- lda_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
#Create term2, a factor ordered by word probability
mutate(term2 = fct_reorder(term, beta))
# Plot term2 and the word probabilities
ggplot(
word_probs,
aes(term2,beta,fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
# Facet the bar plot by topic
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(x = "term")
# select 15 most frequent terms in each topic
word_probs <- lda_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
#Create term2, a factor ordered by word probability
mutate(term2 = fct_reorder(term, beta))
# Plot term2 and the word probabilities
ggplot(
word_probs,
aes(term2,beta,fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
# Facet the bar plot by topic
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(x = "term")
# select 15 most frequent terms in each topic
word_probs <- lda_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
#Create term2, a factor ordered by word probability
mutate(term2 = fct_reorder(term, beta))
# Plot term2 and the word probabilities
ggplot(
word_probs,
aes(term2,beta,fill = as.factor(topic))) +
geom_col(show.legend = FALSE) +
# Facet the bar plot by topic
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(x = "term")
#Create dataframe
df_corpus <- data.frame(text7)
#Create bigrams by separating words in sequences of 2.
bigrams_df <- df_corpus %>%
unnest_tokens(output = bigram,
input = text7,
token = "ngrams",
n = 2)
#Count bigrams
bigrams_df %>%
count(bigram, sort = TRUE)
#Separate words into two columns
bigrams_separated <- bigrams_df %>%
separate(bigram, c("word1", "word2"), sep = " ")
#Remove stopwords
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
#Bigram counts, counts the number of times to words are always together
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
#Create network of bigrams
#filter for common combinations of biwords that appear at least 15 times.
bigram_network <- bigram_counts %>%
filter(n > 15) %>%
graph_from_data_frame()
set.seed(2016)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_network, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 4) +
geom_node_text(aes(label = name), vjust = .7, hjust = 0.1) +
theme_void()
#Create dataframe
df_corpus <- data.frame(text_bigram)
#Create bigrams by separating words in sequences of 2.
bigrams_df <- df_corpus %>%
unnest_tokens(output = bigram,
input = text7,
token = "ngrams",
n = 2)
#Count bigrams
bigrams_df %>%
count(bigram, sort = TRUE)
#Separate words into two columns
bigrams_separated <- bigrams_df %>%
separate(bigram, c("word1", "word2"), sep = " ")
#Remove stopwords
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
#Bigram counts, counts the number of times to words are always together
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
#Create network of bigrams
#filter for common combinations of biwords that appear at least 15 times.
bigram_network <- bigram_counts %>%
filter(n > 15) %>%
graph_from_data_frame()
set.seed(2016)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_network, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 4) +
geom_node_text(aes(label = name), vjust = .7, hjust = 0.1) +
theme_void()
View(bigrams_df)
#Create dataframe
df_corpus <- data.frame(text_bigram)
#Create bigrams by separating words in sequences of 2.
bigrams_df <- df_corpus %>%
unnest_tokens(output = bigram,
input = text7,
token = "ngrams",
n = 2)
View(df_corpus)
#Create bigrams by separating words in sequences of 2.
bigrams_df <- df_corpus %>%
unnest_tokens(output = bigram,
input = text_bigram,
token = "ngrams",
n = 2)
#Count bigrams
bigrams_df %>%
count(bigram, sort = TRUE)
#Separate words into two columns
bigrams_separated <- bigrams_df %>%
separate(bigram, c("word1", "word2"), sep = " ")
#Remove stopwords
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
#Bigram counts, counts the number of times to words are always together
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
#Create network of bigrams
#filter for common combinations of biwords that appear at least 15 times.
bigram_network <- bigram_counts %>%
filter(n > 15) %>%
graph_from_data_frame()
set.seed(2016)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_network, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 4) +
geom_node_text(aes(label = name), vjust = .7, hjust = 0.1) +
theme_void()
#Create dataframe
df_corpus <- data.frame(text_bigram)
#Create bigrams by separating words in sequences of 2.
bigrams_df <- df_corpus %>%
unnest_tokens(output = bigram,
input = text_bigram,
token = "ngrams",
n = 2)
#Count bigrams
bigrams_df %>%
count(bigram, sort = TRUE)
#Separate words into two columns
bigrams_separated <- bigrams_df %>%
separate(bigram, c("word1", "word2"), sep = " ")
#Count the number of times two words are always together
bigram_counts <- bigrams_separated %>%
count(word1, word2, sort = TRUE)
#Create network of bigrams
#filter for common combinations of biwords that appear at least 15 times.
bigram_network <- bigram_counts %>%
filter(n > 15) %>%
graph_from_data_frame()
set.seed(2016)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_network, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 4) +
geom_node_text(aes(label = name), vjust = .7, hjust = 0.1) +
theme_void()
#Create dataframe
df_corpus <- data.frame(text_bigram)
#Create bigrams by separating words in sequences of 2.
bigrams_df <- df_corpus %>%
unnest_tokens(output = bigram,
input = text_bigram,
token = "ngrams",
n = 2)
#Count bigrams
bigrams_df %>%
count(bigram, sort = TRUE)
#Separate words into two columns
bigrams_separated <- bigrams_df %>%
separate(bigram, c("word1", "word2"), sep = " ")
#Remove stopwords
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
#Count the number of times two words are always together
bigram_counts <- bigrams_separated %>%
count(word1, word2, sort = TRUE)
#Create network of bigrams
#filter for common combinations of biwords that appear at least 15 times.
bigram_network <- bigram_counts %>%
filter(n > 15) %>%
graph_from_data_frame()
set.seed(2016)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_network, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 4) +
geom_node_text(aes(label = name), vjust = .7, hjust = 0.1) +
theme_void()
#Create dataframe
df_corpus <- data.frame(text_bigram)
#Create bigrams by separating words in sequences of 2.
bigrams_df <- df_corpus %>%
unnest_tokens(output = bigram,
input = text_bigram,
token = "ngrams",
n = 2)
#Count bigrams
bigrams_df %>%
count(bigram, sort = TRUE)
#Separate words into two columns
bigrams_separated <- bigrams_df %>%
separate(bigram, c("word1", "word2"), sep = " ")
#Remove stopwords
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word)
#Count the number of times two words are always together
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
#Create network of bigrams
#filter for common combinations of biwords that appear at least 15 times.
bigram_network <- bigram_counts %>%
filter(n > 15) %>%
graph_from_data_frame()
set.seed(2016)
a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
ggraph(bigram_network, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.07, 'inches')) +
geom_node_point(color = "lightblue", size = 4) +
geom_node_text(aes(label = name), vjust = .7, hjust = 0.1) +
theme_void()
library(quanteda)
library(igraph)
library(ggraph)
library(tidyverse)
library(tidyr)
#library(broom)
setwd("G:/O meu disco/Thesis/Artigo Revisao Systematica - LDA/Github_Review_textmining/Review_Papers_Text_Mining")
knitr::opts_chunk$set(echo = TRUE)
df <- data.frame(systematicreview)
