text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
#A78_85 <- tm_map(A78_85, PlainTextDocument)
#strings <- c("x")
#abstracts <- str_replace_all(A78_85, strings, "x")
A78_85final <- data.frame(A78_85)
tokenizing_abstract <- A78_85final %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
View(A78_85final)
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
A78_85 <- tm_map(text3, PlainTextDocument)
systematicreview <- read_excel("Data/table_systematic_review.xlsx")
df <- data.frame(systematicreview)
df <- df[,-c(7)]
df <- subset(df, df[2]=="1")
A78_85 <- df[df$Year >= 1978 & df$Year<=1985,]
View(A78_85)
A78_85 <- A78_85$Abstract
A78_85 <- A78_85$Abstract
A78_85 <- data.frame(A78_85$Abstract)
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
corpus <- Corpus(VectorSource(A78_85))
View(corpus)
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
corpus <- Corpus(VectorSource(A78_85))
dtm <- DocumentTermMatrix(corpus)
View(dtm)
View(dtm)
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
corpus <- Corpus(VectorSource(A78_85))
dtm <- DocumentTermMatrix(corpus)
dtm.matrix <- as.matrix(dtm)
wordcount <- colSums(dtm.matrix)
systematicreview <- read_excel("Data/table_systematic_review.xlsx")
df <- data.frame(systematicreview)
df <- df[,-c(7)]
df <- subset(df, df[2]=="1")
A78_85 <- df[df$Year >= 1978 & df$Year<=1985,]
View(A78_85)
A78_85 <- df[df$Year >= 1978 & df$Year<=1985 & df$Abstract,]
View(A78_85)
A78_85 <- A78_85[,8]
systematicreview <- read_excel("Data/table_systematic_review.xlsx")
df <- data.frame(systematicreview)
df <- df[,-c(7)]
df <- subset(df, df[2]=="1")
A78_85 <- df[df$Year >= 1978 & df$Year<=1985,]
View(A78_85)
library(tm)
library(NLP)
library(tidyverse)
library(tidytext)
library(readxl)
library(textstem)
systematicreview <- read_excel("Data/table_systematic_review.xlsx")
df <- data.frame(systematicreview)
df <- df[,-c(7)]
df <- subset(df, df[2]=="1")
A78_85 <- df[df$Year >= 1978 & df$Year<=1985,]
A78_85 <- A78_85[,-c(1,2,3,4,5,6,7)]
A78_85 <- df[df$Year >= 1978 & df$Year<=1985,]
A78_85 <- data.frame(A78_85[,-c(1,2,3,4,5,6,7)])
View(A78_85)
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
A78_85 <- tm_map(A78_85, PlainTextDocument)
A78_85 <- df[df$Year >= 1978 & df$Year<=1985,]
A78_85 <- data.frame(A78_85[,-c(1,2,3,4,5,6,7)])
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
A78_85 <- Corpus(A78_85)
A78_85 <- A78_85$Abstract
df <- df[,-c(7)]
df <- subset(df, df[2]=="1")
A78_85 <- df[df$Year >= 1978 & df$Year<=1985,]
A78_85 <- A78_85$Abstract
A78_85 <- tm_map(A78_85, PlainTextDocument)
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(NLP)
library(tidyverse)
library(tidytext)
library(readxl)
library(textstem)
install.packages("koRpus")
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(NLP)
library(tidyverse)
library(tidytext)
library(readxl)
library(textstem)
library(koRpus)
systematicreview <- read_excel("Data/table_systematic_review.xlsx")
df <- data.frame(systematicreview)
df <- df[,-c(7)]
df <- subset(df, df[2]=="1")
A78_85 <- df[df$Year >= 1978 & df$Year<=1985,]
A78_85 <- A78_85$Abstract
A78_85 <- tm_map(A78_85, PlainTextDocument)
A78_85 <- A78_85$Abstract
A78_85 <- data.frame(df[df$Year >= 1978 & df$Year<=1985,])
View(A78_85)
A78_85 <- A78_85$Abstract
A78_85 <- tm_map(A78_85, PlainTextDocument)
A78_85 <- data.frame(A78_85$Abstract)
A78_85 <- data.frame(df[df$Year >= 1978 & df$Year<=1985,])
A78_85 <- A78_85[,]
View(A78_85)
A78_85 <- data.frame(df[df$Year >= 1978 & df$Year<=1985,])
A78_85 <- A78_85[,8]
View(df)
A78_85 <- tm_map(A78_85, PlainTextDocument)
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
A78_85 <- tm_map(A78_85, PlainTextDocument)
A78_85 <- VCorpus(A78_85)
A78_85 <- data.frame(df[df$Year >= 1978 & df$Year<=1985,])
A78_85 <- A78_85[,8]
tokenizing_abstract <- A78_85 %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstract) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
A78_85 <- data.frame(df[df$Year >= 1978 & df$Year<=1985,])
A78_85 <- data.frame(A78_85[,8])
View(A78_85)
tokenizing_abstract <- A78_85 %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstract) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
A78_85 <- data.frame(df[df$Year >= 1978 & df$Year<=1985,])
A78_85 <- data.frame(A78_85[,8])
colnames(A78_85) <- c("abstract")
View(A78_85)
tokenizing_abstract <- A78_85 %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstract) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
View(tokenizing_abstract)
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
A78_85 <- tm_map(A78_85, PlainTextDocument)
A78_85 <- data.frame(df[df$Year >= 1978 & df$Year<=1985,])
A78_85 <- data.frame(A78_85[,8])
colnames(A78_85) <- c("abstract")
tokenizing_abstract <- A78_85 %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstract) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
A78_85 <- tm_map(A78_85, PlainTextDocument)
A78_85 <- data.frame(df[df$Year >= 1978 & df$Year<=1985,])
A78_85 <- data.frame(A78_85[,8])
colnames(A78_85) <- c("abstract")
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
tokenizing_abstract <- A78_85 %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstract) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
A78_85 <- data.frame(A78_85)
View(A78_85)
View(A78_85)
A78_85 <- data.frame(df[df$Year >= 1978 & df$Year<=1985,])
A78_85 <- data.frame(A78_85[,8])
colnames(A78_85) <- c("abstract")
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
A78_85 <- data.frame(A78_85)
tokenizing_abstract <- A78_85 %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstract) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
View(A78_85)
A78_85 <- data.frame(df[df$Year >= 1978 & df$Year<=1985,])
A78_85 <- data.frame(A78_85[,8])
colnames(A78_85) <- c("abstract")
A78_85 <- data.frame(df[df$Year >= 1978 & df$Year<=1985,])
A78_85 <- A78_85[,8]
#colnames(A78_85) <- c("abstract")
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
A78_85 <- data.frame(A78_85)
colnames(A78_85) <- c("abstract")
View(A78_85)
tokenizing_abstract <- A78_85 %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstract) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
View(df)
systematicreview <- read_excel("Data/table_systematic_review.xlsx")
df <- data.frame(systematicreview)
skim(df)
df <- subset(df, df[2]=="1")
library(tm)
library(NLP)
library(tidyverse)
library(tidytext)
library(readxl)
library(textstem)
library(koRpus)
library(skimr)
systematicreview <- read_excel("Data/table_systematic_review.xlsx")
df <- data.frame(systematicreview)
skim(df)
df <- subset(df, df[2]=="1")
A78_85 <- data.frame(df[df$Year >= 1978 & df$Year<=1985,])
A78_85 <- A78_85[,8]
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
A78_85 <- data.frame(A78_85)
colnames(A78_85) <- c("abstract")
tokenizing_abstract <- A78_85 %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstract) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
adicional_stopwords <- tribble(
~word,~lexicon,
"study", "CUSTOM",
"paper", "CUSTOM",
"article", "CUSTOM",
"based", "CUSTOM",
"results", "CUSTOM",
"information","CUSTOM",
"research", "CUSTOM")
stop_words2 <- stop_words %>%
bind_rows(adicional_stopwords)
tokenizing_abstract <- A78_85 %>%
mutate(id = row_number()) %>%
unnest_tokens(word, abstract) %>%
anti_join(stop_words2)
word_counts <- tokenizing_abstract %>%
count(word) %>%
filter(n>1) %>% #Filter words that appear more than "n" times
mutate(word2 = fct_reorder(word, n)) %>%
arrange(desc(n))
ggplot(
word_counts, aes (x = word2, y = n/max(n), fill = "red")
) +
geom_col() +
coord_flip() +
labs(
title = "Most frequent words of filtered abstracts: 1978 - 1985",
x = "Words", y ="Normalized frequency - Total of 2 papers")
tokenizing_abstract <- A78_85 %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstract) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
df <- subset(df, df[2]=="1")
df <- data.frame(systematicreview)
skim(df)
df <- subset(df, df[2]=="1")
A78_85 <- data.frame(df[df$Year >= 1978 & df$Year<=1985,])
View(A78_85)
A78_85 <- A78_85[,8]
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
A78_85 <- data.frame(A78_85)
colnames(A78_85) <- c("abstract")
View(A78_85)
A78_85 <- A78_85[,8]
systematicreview <- read_excel("Data/table_systematic_review.xlsx")
df <- data.frame(systematicreview)
skim(df)
library(tm)
library(NLP)
library(tidyverse)
library(tidytext)
library(readxl)
library(textstem)
library(koRpus)
library(skimr)
systematicreview <- read_excel("Data/table_systematic_review.xlsx")
df <- data.frame(systematicreview)
skim(df)
View(df)
df <- subset(df, df[2]=="1")
A78_85 <- data.frame(df[df$Year >= 1978 & df$Year<=1985,])
A78_85 <- A78_85[,9]
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
A78_85 <- data.frame(A78_85)
colnames(A78_85) <- c("abstract")
tokenizing_abstract <- A78_85 %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstract) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
adicional_stopwords <- tribble(
~word,~lexicon,
"study", "CUSTOM",
"paper", "CUSTOM",
"article", "CUSTOM",
"based", "CUSTOM",
"results", "CUSTOM",
"information","CUSTOM",
"research", "CUSTOM")
stop_words2 <- stop_words %>%
bind_rows(adicional_stopwords)
tokenizing_abstract <- A78_85 %>%
mutate(id = row_number()) %>%
unnest_tokens(word, abstract) %>%
anti_join(stop_words2)
word_counts <- tokenizing_abstract %>%
count(word) %>%
filter(n>1) %>% #Filter words that appear more than "n" times
mutate(word2 = fct_reorder(word, n)) %>%
arrange(desc(n))
ggplot(
word_counts, aes (x = word2, y = n/max(n), fill = "red")
) +
geom_col() +
coord_flip() +
labs(
title = "Most frequent words of filtered abstracts: 1978 - 1985",
x = "Words", y ="Normalized frequency - Total of 2 papers")
View(df)
library(tm)
library(NLP)
library(tidyverse)
library(tidytext)
library(readxl)
library(textstem)
library(skimr)
library(kableExtra)
install.packages("kableExtra")
df <- data.frame(systematicreview)
df %>%
kbl() %>%
kable_styling()
df <- data.frame(systematicreview)
kbl(df) %>%
kable_styling(bootstrap_options = "striped", font_size = 7)
library(tm)
library(NLP)
library(tidyverse)
library(tidytext)
library(readxl)
library(textstem)
library(skimr)
library(kableExtra)
systematicreview <- read_excel("Data/table_systematic_review.xlsx")
df <- data.frame(systematicreview)
kbl(df) %>%
kable_styling(bootstrap_options = "striped", font_size = 7)
install.packages("webshot")
knitr::opts_chunk$set(echo = TRUE, )
