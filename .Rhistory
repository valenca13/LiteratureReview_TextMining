#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A86_90 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
A86_90final <- data.frame(A86_90)
colnames(A86_90final) <- c("abstracts")
#Tokenize words from abstracts
tokenizing_abstract <- A86_90final %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstracts) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
adicional_stopwords <- tribble(
~word,~lexicon,
"study", "CUSTOM",
"paper", "CUSTOM",
"article", "CUSTOM",
"based", "CUSTOM",
"results", "CUSTOM",
"discussion", "CUSTOM",
"information", "CUSTOM",
"research", "CUSTOM")
stop_words2 <- stop_words %>%
bind_rows(adicional_stopwords)
#Add new stopwords and repeat the tokenization process
tokenizing_abstract <- A86_90final %>%
mutate(id = row_number()) %>%
unnest_tokens(word, abstracts) %>%
anti_join(stop_words2)
#Frequency of words
word_counts <- tokenizing_abstract %>%
count(word) %>%
filter(n>1) %>% #Filter words that appear more than "n" times
mutate(word2 = fct_reorder(word, n)) %>%
arrange(desc(n))
#Plot word count
ggplot(
word_counts, aes (x = word2, y = n/max(n), fill = "red")
) +
geom_col() +
coord_flip() +
labs(
title = "Most frequent words of filtered abstracts: 1986 - 1990",
x = "Words", y ="Normalized frequency - Total of 1 paper"
)
# Select papers from 1991 - 1995
A91_95 <- df[df$Year >= 1991 & df$Year<=1995,]
# Select only the abstract
A91_95 <- A91_95[,9]
#Data Cleaning
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A91_95)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A91_95 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
A91_95final <- data.frame(A91_95)
colnames(A91_95final) <- c("abstracts")
#Tokenize words from abstracts
tokenizing_abstract <- A91_95final %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstracts) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
adicional_stopwords <- tribble(
~word,~lexicon,
"study", "CUSTOM",
"paper", "CUSTOM",
"article", "CUSTOM",
"based", "CUSTOM",
"information", "CUSTOM",
"discussion", "CUSTOM",
"results", "CUSTOM",
"research", "CUSTOM")
stop_words2 <- stop_words %>%
bind_rows(adicional_stopwords)
#Add new stopwords and repeat the tokenization process
tokenizing_abstract <- A91_95final %>%
mutate(id = row_number()) %>%
unnest_tokens(word, abstracts) %>%
anti_join(stop_words2)
#Frequency of words
word_counts <- tokenizing_abstract %>%
count(word) %>%
filter(n>3) %>% #Filter words that appear more than "n" times
mutate(word2 = fct_reorder(word, n)) %>%
arrange(desc(n))
#Plot word count
ggplot(
word_counts, aes (x = word2, y = n/max(n), fill = "red")
) +
geom_col() +
coord_flip() +
labs(
title = "Most frequent words of filtered abstracts: 1991 - 1995",
x = "Words", y ="Normalized frequency - Total of 3 papers"
)
# Select papers from 1996 - 2000
A96_00 <- df[df$Year >= 1996 & df$Year<=2000,]
# Select only the abstract
A96_00 <- A96_00[,9]
#Data Cleaning
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A96_00)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A96_00 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
A96_00final <- data.frame(A96_00)
colnames(A96_00final) <- c("abstracts")
#Tokenize words from abstracts
tokenizing_abstract <- A96_00final %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstracts) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
adicional_stopwords <- tribble(
~word,~lexicon,
"study", "CUSTOM",
"paper", "CUSTOM",
"article", "CUSTOM",
"based", "CUSTOM",
"information", "CUSTOM",
"discussion", "CUSTOM",
"results", "CUSTOM",
"research", "CUSTOM")
stop_words2 <- stop_words %>%
bind_rows(adicional_stopwords)
#Add new stopwords and repeat the tokenization process
tokenizing_abstract <- A96_00final %>%
mutate(id = row_number()) %>%
unnest_tokens(word, abstracts) %>%
anti_join(stop_words2)
#Frequency of words
word_counts <- tokenizing_abstract %>%
count(word) %>%
filter(n>8) %>% #Filter words that appear more than "n" times
mutate(word2 = fct_reorder(word, n)) %>%
arrange(desc(n))
#Plot word count
ggplot(
word_counts, aes (x = word2, y = n/max(n), fill = "red")
) +
geom_col() +
coord_flip() +
labs(
title = "Most frequent words of filtered abstracts: 1996 - 2000",
x = "Words", y ="Normalized frequency - Total of 19 papers"
)
# Select papers from 2001 - 2005
A01_05 <- df[df$Year >= 2001 & df$Year<=2005,]
# Select only the abstract
A01_05 <- A01_05[,9]
#Data Cleaning
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A01_05)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A01_05 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
A01_05final <- data.frame(A01_05)
colnames(A01_05final) <- c("abstracts")
#Tokenize words from abstracts
tokenizing_abstract <- A01_05final %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstracts) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
adicional_stopwords <- tribble(
~word,~lexicon,
"study", "CUSTOM",
"paper", "CUSTOM",
"article", "CUSTOM",
"based", "CUSTOM",
"results", "CUSTOM",
"research", "CUSTOM")
stop_words2 <- stop_words %>%
bind_rows(adicional_stopwords)
#Add new stopwords and repeat the tokenization process
tokenizing_abstract <- A01_05final %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstracts) %>%
anti_join(stop_words2)
#Frequency of words
word_counts <- tokenizing_abstract %>%
count(word) %>%
filter(n>7) %>% #Filter words that appear more than "n" times
mutate(word2 = fct_reorder(word, n)) %>%
arrange(desc(n))
#Plot word count
ggplot(
word_counts, aes (x = word2, y = n/max(n), fill = "red")
) +
geom_col() +
coord_flip() +
labs(
title = "Most frequent words of filtered abstracts: 2001 - 2005",
x = "Words", y ="Normalized frequency - Total of 13 papers"
)
#Select papers from 2006 - 2010
A06_10 <- df[df$Year >= 2006 & df$Year<=2010,]
# Select only the abstract
A06_10 <- A06_10[,9]
#Data Cleaning
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A06_10)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A06_10 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
A06_10final <- data.frame(A06_10)
colnames(A06_10final) <- c("abstracts")
#Tokenize words from abstracts
tokenizing_abstract <- A06_10final %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstracts) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
adicional_stopwords <- tribble(
~word,~lexicon,
"study", "CUSTOM",
"paper", "CUSTOM",
"article", "CUSTOM",
"based", "CUSTOM",
"results", "CUSTOM",
"processes", "CUSTOM",
"research", "CUSTOM",
"datum", "CUSTOM")
stop_words2 <- stop_words %>%
bind_rows(adicional_stopwords)
#Add new stopwords and repeat the tokenization process
tokenizing_abstract <- A06_10final %>%
mutate(id = row_number()) %>%
unnest_tokens(word, abstracts) %>%
anti_join(stop_words2)
#Frequency of words
word_counts <- tokenizing_abstract %>%
count(word) %>%
filter(n>10) %>% #Filter words that appear more than "n" times
mutate(word2 = fct_reorder(word, n)) %>%
arrange(desc(n))
#Plot word count
ggplot(
word_counts, aes (x = word2, y = n/max(n), fill = "red")
) +
geom_col() +
coord_flip() +
labs(
title = "Most frequent words of filtered abstracts: 2006 - 2010",
x = "Words", y ="Normalized frequency - Total of 31 papers"
)
# Select papers from 2011 - 2015
A11_15 <- df[df$Year >= 2011 & df$Year<= 2015,]
# Select only the abstract
A11_15 <- A11_15[,9]
#Data Cleaning
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A11_15)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A11_15 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
A11_15final <- data.frame(A11_15)
colnames(A11_15final) <- c("abstracts")
#Tokenize words from abstracts
tokenizing_abstract <- A11_15final %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstracts) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
adicional_stopwords <- tribble(
~word,~lexicon,
"study", "CUSTOM",
"paper", "CUSTOM",
"article", "CUSTOM",
"based", "CUSTOM",
"results", "CUSTOM",
"research", "CUSTOM",
"datum", "CUSTOM")
stop_words2 <- stop_words %>%
bind_rows(adicional_stopwords)
#Add new stopwords and repeat the tokenization process
tokenizing_abstract <- A11_15final %>%
mutate(id = row_number()) %>%
unnest_tokens(word, abstracts) %>%
anti_join(stop_words2)
#Frequency of words
word_counts <- tokenizing_abstract %>%
count(word) %>%
filter(n>40) %>% #Filter words that appear more than "n" times
mutate(word2 = fct_reorder(word, n)) %>%
arrange(desc(n))
#Plot word count
ggplot(
word_counts, aes (x = word2, y = n/max(n), fill = "red")
) +
geom_col() +
coord_flip() +
labs(
title = "Most frequent words of filtered abstracts: 2011 - 2015",
x = "Words", y ="Normalized frequency - Total of 103 papers"
)
# Select papers from 2016 - 2020
A16_20 <- df[df$Year >= 2016 & df$Year<=2020,]
# Select only the abstract
A16_20 <- A16_20[,9]
#Data Cleaning
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A16_20)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A16_20 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
A16_20final <- data.frame(A16_20)
colnames(A16_20final) <- c("abstracts")
#Tokenize words from abstracts
tokenizing_abstract <- A16_20final %>%
mutate(id = row_number()) %>% #Create new id for each abstract
unnest_tokens(word, abstracts) %>%
anti_join(stop_words) #Remove stopwords (english package of stopwords)
adicional_stopwords <- tribble(
~word,~lexicon,
"study", "CUSTOM",
"paper", "CUSTOM",
"article", "CUSTOM",
"based", "CUSTOM",
"information","CUSTOM",
"analysis", "CUSTOM",
"discussion","CUSTOM",
"results", "CUSTOM",
"research", "CUSTOM",
"datum", "CUSTOM")
stop_words2 <- stop_words %>%
bind_rows(adicional_stopwords)
#Add new stopwords and repeat the tokenization process
tokenizing_abstract <- A16_20final %>%
mutate(id = row_number()) %>%
unnest_tokens(word, abstracts) %>%
anti_join(stop_words2)
#Frequency of words
word_counts <- tokenizing_abstract %>%
count(word) %>%
filter(n>130) %>% #Filter words that appear more than "n" times
mutate(word2 = fct_reorder(word, n)) %>%
arrange(desc(n))
#Plot word count
ggplot(
word_counts, aes (x = word2, y = n/max(n), fill = "red")
) +
geom_col() +
coord_flip() +
labs(
title = "Most frequent words of filtered abstracts: 2016 - 2020",
x = "Words", y ="Normalized frequency - Total of 326 papers"
)
library(tm)
library(nlp)
library(stringr)
library(topicmodels)
library(tidytext)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(SnowballC)
library(textstem)
folder <- "Data\Full papers for LDA_bigrams"
folder <- "Data\\Full papers for LDA_bigrams"
filelist <- list.files(folder, pattern = ".txt") #select only documents ".txt"
filelist <- paste(folder, "\\", filelist, sep="") #Join documents.
x <- lapply(filelist, FUN = readLines) #Considers each line as a different element (document).
docs <- lapply(x, FUN = paste, collapse = " ")
View(docs)
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", docs)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3)
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
text6 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)
adicional_stopwords <- c("good","represent", "present", "different","london", "may","datum","taipei", "numb", "much", "one", "two", "can", "fig", "will", "arm", "along", "xpj", "figure", "thus","aviv", "tel", "dsc","dscs","traf","also","study", stopwords("en"))
#remove stopwords
text7 <- removeWords(text6, adicional_stopwords)
#A Good practice is to visualize the corpus every now and then.
writeLines(as.character(text7[[1]]))
# Remove words for bigrams
new_stopwords <- c("ow", "ï", "exible", "cantly","wick", "â", "exibility", "uence", "uences", "ned")
text_bigram <- removeWords(text7, new_stopwords)
# Create corpus from vector
corpus <- Corpus(VectorSource(text7))
# Create document term matrix
dtm <- DocumentTermMatrix(corpus)
#str(dtm)
#count top ten words
dtm.matrix <- as.matrix(dtm)
wordcount <- colSums(dtm.matrix)
topten <- head(sort(wordcount, decreasing=TRUE), 10)
print(topten)
#Number of topics (k) is defined prior to the model.
k <- 6
#Run LDA using Gibbs sampling
ldaOut <- LDA(dtm,
k,
method="Gibbs",
control=list(seed = 42))
lda_topics <- ldaOut %>%
tidy(matrix = "beta") %>%
arrange(desc(beta))
lda_topics <- LDA(corpus,
k,
method="Gibbs",
control=list(seed = 42))
glimpse(ldaOut)
# select 15 most frequent terms in each topic
word_probs <- lda_topics %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
#Create term2, a factor ordered by word probability
mutate(term2 = fct_reorder(term, beta))
# Plot term2 and the word probabilities
ggplot(
word_probs,
aes(term2,beta,fill = as.factor(topic))
) + geom_col(show.legend = FALSE) +
# Facet the bar plot by topic
facet_wrap(~ topic, scales = "free") +
coord_flip() +
labs(x = "term")
library(quanteda)
library(igraph)
library(ggraph)
library(tidyr)
#Covert each paper to a line
t_corpus <- text_bigram %>% tidy()
#create dataframe
df_corpus <- data.frame(t_corpus)
#Create biagrams by separating words in sequences of 2.
bigrams_df <- df_corpus %>%
unnest_tokens(output = bigram,
input = x,
token = "ngrams",
n = 2)
install.packages("quanteda")
install.packages("igraph")
install.packages("ggraph")
install.packages("tidyr")
library(quanteda)
library(igraph)
library(ggraph)
library(tidyr)
#Covert each paper to a line
t_corpus <- text_bigram %>% tidy()
library(tidyr)
#Covert each paper to a line
t_corpus <- text_bigram %>% tidy()
library(tidyverse)
#Covert each paper to a line
t_corpus <- text_bigram %>% tidy()
install.packages("broom")
library(broom)
#Covert each paper to a line
t_corpus <- text_bigram %>% tidy()
#create dataframe
df_corpus <- data.frame(t_corpus)
library(broom)
#Covert each paper to a line
t_corpus <- text_bigram %>% tidy()
library(quanteda)
library(igraph)
library(ggraph)
library(tidyverse)
library(tidyr)
library(broom)
#Covert each paper to a line
t_corpus <- text_bigram %>% tidy()
