---
title: "Specific paper analysis - Topic modelling and bigrams"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### 1. Import libraries

```{r message=FALSE, warning=FALSE}
library(tm)
library(NLP)
library(stringr)
library(topicmodels)
library(tidytext)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(SnowballC)
library(textstem)
```


#### 2. Import papers

After downloading the papers we converted the ".pdf" files to ".txt" files and manually removed the information of the authors and journal, acknowledgments, funding, supplemental material, disclosure statement, and references.

> **Note:`** This prior data cleaning ensures that the information provided by the model was only from the paper's text body.


```{r}

folder <- "Data\\Full_papers"

filelist <- list.files(folder, pattern = ".txt") #select only documents ".txt"

filelist <- paste(folder, "\\", filelist, sep="") #Join documents.  

x <- lapply(filelist, FUN = readLines) #Considers each line as a different element (document). 

docs <- lapply(x, FUN = paste, collapse = " ")
```

#### 3. Data cleaning

```{r}
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", docs)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words 
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3) 
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
text6 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)

adicional_stopwords <- c("good","represent", "present", "different","london", "may","datum","taipei", "numb", "much", "one", "two", "can", "fig", "will", "arm", "along", "xpj", "figure", "thus","aviv", "tel", "dsc","dscs","traf","also","study", stopwords("en"))
#remove stopwords
text7 <- removeWords(text6, adicional_stopwords)

# Remove words for bigrams
new_stopwords <- c("ow", "exible", "cantly","wick", "exibility", "uence", "uences", "ned")
text_bigram <- removeWords(text7, new_stopwords)
```

#### 4. Create corpus

A corpus is a collection of texts. In this case, the corpus is going to group all the papers.

```{r}
corpus <- Corpus(VectorSource(text7))
```

#### 5. Topic modelling - Latent Dirichlet Allocation (LDA)

Topic modelling is a Natural Language Processing technique that discovers patterns of words (topics) of a collection of documents.

##### a) Create Document-Term-Matrix (DTM)

```{r}
dtm <- DocumentTermMatrix(corpus) 
str(dtm)
```

##### b) Take a look at the ten words that appear more frequently
```{r}
dtm.matrix <- as.matrix(dtm)
wordcount <- colSums(dtm.matrix)
topten <- head(sort(wordcount, decreasing=TRUE), 10)
print(topten)
```

##### c) Define the number of topics (k) 
```{r}
k <- 6
```

> **Note:** The number of topics is defined prior to the model. 

##### d) Run LDA using Gibbs sampling

```{r}
ldaOut <- LDA(dtm,
             k, 
             method="Gibbs", 
             control=list(seed = 42)) 

lda_topics <- ldaOut %>%
  tidy(matrix = "beta") %>%
          arrange(desc(beta))
```

##### e) Select 15 most frequent terms in each topic.

>**Note:** The number of terms you want to display depends on your analysis. 

```{r}

word_probs <- lda_topics %>%
  group_by(topic) %>%
  top_n(15, beta) %>%
  ungroup() %>%
  #Create term2, a factor ordered by word probability
  mutate(term2 = fct_reorder(term, beta))
```

##### f) Plot the topics

```{r}
ggplot(
  word_probs,
  aes(term2,beta,fill = as.factor(topic))
) + geom_col(show.legend = FALSE) +
  # Facet the bar plot by topic
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  labs(x = "term")
```


>**Note:** If you run the algorithm many times, the topics will maintain the same. Nonetheless, the order of the topics may appear differently. 

#### 6. Bigrams

##### a) Import Libraries

```{r message=FALSE, warning=FALSE}
library(quanteda)
library(igraph)
library(ggraph)
library(tidyverse)
library(tidyr)
library(broom)
```

##### b) Create dataframe

```{r}
df_corpus <- data.frame(text_bigram)
```

##### c) Create bigrams by separating words in sequences of 2

```{r}
bigrams_df <- df_corpus %>%
  unnest_tokens(output = bigram,
                input = text_bigram,
                token = "ngrams",
                n = 2)
```

##### d) Count bigrams

```{r echo=T, results='hide'}
bigrams_df %>%
  count(bigram, sort = TRUE)
```

##### e) Separate words into two columns

```{r}
bigrams_separated <- bigrams_df %>%
  separate(bigram, c("word1", "word2"), sep = " ")
```

##### f) Remove stopwords that may have remained in the corpus

```{r}
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)
```

##### g) Count the number of times two words are always together

```{r}
bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE)
```

##### h) Create network of bigrams

```{r}
bigram_network <- bigram_counts %>%
  filter(n > 15) %>% #filter for the most common combinations of bigrams that appear at least 15 times.
  graph_from_data_frame()

set.seed(2016)

a <- grid::arrow(type = "closed", length = unit(.07, "inches"))

ggraph(bigram_network, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.03, 'inches')) +
  geom_node_point(color = "lightblue", size = 2) +
  geom_node_text(aes(label = name), vjust = .7, hjust = 0.1) +
  theme_void()
```

