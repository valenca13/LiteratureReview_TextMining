---
title: "Historical Analysis of abstracts"
output: github_document
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This code was elaborated to evaluate the evolution of topics discussed in the literature throughout the years. We divide the abstracts of the papers in different corpus according to the year they were published and perform a word frequency count. 

#### Import libraries

```{r message=FALSE, warning=FALSE}
library(tm)
library(NLP)
library(tidyverse)
library(tidytext)
library(readxl)
library(textstem)
library(skimr)
library(kableExtra)
```


#### 1. Import table

```{r}
systematicreview <- read_excel("Data/table_systematic_review.xlsx")
```

* `ID`: identification of paper.  

* `Historical_Analysis`: In the filtering process, the papers selected for the historical analysis were assigned in this column as "1". Papers that were totally out of scope of the objective of the literature review were assigned as "0" and not included in this analysis.   

* `Specific_Analysis`: In the filtering process, papers that were selected for full paper analysis through topic modelling, bigrams and qualitative analysis were assigned as "1". Otherwise, paper were assigned as "0" and were not included in the full paper analysis.   

* `Keyword`: The keywords used in the paper.

* `Journal`: The journal the paper was published.

* `Title`: The title of the published paper.  

* `Authors`: The respective authors of the published paper.  

* `Year`: The year the paper was published.  

* `Abstract`: The abstract of the published paper.

#### 2. Convert the table into a dataframe and take a look at the first 8 papers

```{r}
df <- data.frame(systematicreview)

kbl(df%>% slice_head(n=8)) %>%
  kable_styling(bootstrap_options = "striped", font_size = 8, fixed_thead = T) %>%
  kable_minimal()
```


>**Note:** In this code we we want to access the papers that were considered coherent for performing an historical analysis. Therefore, the papers that were filtered "Historical_Analysis" = 1.  

#### 3. Filter only papers that were considered relevant for the analysis. "Historical_Analysis" = 1.

```{r}
df <- subset(df, df[2]=="1") 
```

#### 4. Select the period you want to analyze

#### Papers from 1978 to 1985 

```{r}
A78_85 <- data.frame(df[df$Year >= 1978 & df$Year<=1985,])

```

##### Select only the abstract

```{r}
A78_85 <- A78_85[,9]
```

##### Data Cleaning 

```{r}
#Remove punctuation
text <- gsub(pattern = "\\W", replace = " ", A78_85)
#Remove Numbers (digits)
text2 <- gsub(pattern = "\\d", replace = " ", text)
#Lowercase words
text3 <- tolower(text2)
#remove single words 
text4 <- gsub(pattern = "\\b[A-z]\\b{1}", replace = " ", text3) 
#Remove whitespace
text5 <- stripWhitespace(text4)
#Lematize terms in its dictionary form
A78_85 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)

A78_85 <- data.frame(A78_85)
colnames(A78_85) <- c("abstract")
```

##### Tokenize words from abstracts

```{r}
tokenizing_abstract <- A78_85 %>%
  mutate(id = row_number()) %>% #Create new id for each abstract
  unnest_tokens(word, abstract) %>%
  anti_join(stop_words) #Remove stopwords (english package of stopwords)
```

##### Select additional stopwords and repeat the tokenization process

```{r}
adicional_stopwords <- tribble(
  ~word,~lexicon,
  "study", "CUSTOM",
  "paper", "CUSTOM",
  "article", "CUSTOM",
  "based", "CUSTOM",
  "results", "CUSTOM",
  "information","CUSTOM",
  "research", "CUSTOM")

stop_words2 <- stop_words %>%
  bind_rows(adicional_stopwords) 

tokenizing_abstract <- A78_85 %>%
  mutate(id = row_number()) %>% 
  unnest_tokens(word, abstract) %>%
  anti_join(stop_words2) 
```

#### Frequency of words

```{r}
word_counts <- tokenizing_abstract %>%
  count(word) %>%
  filter(n>1) %>% #Filter words that appear more than "n" times
  mutate(word2 = fct_reorder(word, n)) %>% 
  arrange(desc(n)) 
```

#### Plot word count

```{r}
ggplot(
  word_counts, aes (x = word2, y = n/max(n), fill = "red")
) +
  geom_col() + 
  coord_flip() + 
  labs(
    title = "Most frequent words of filtered abstracts: 1978 - 1985",
    x = "Words", y ="Normalized frequency - Total of 2 papers")
```

> The process is the same for other periods of time. At the end, it is possible to compare the most frequent words in abstracts of papers from different periods. 

In case you want to check how to perform this algorithm in the other periods, the complete code that was used for the paper can be assessed through the [script file](Scripts/Script_Historical_Analysis.R). 




