"0","#Remove punctuation"
"0","text <- gsub(pattern = ""\\W"", replace = "" "", docs)"
"0","#Remove Numbers (digits)"
"0","text2 <- gsub(pattern = ""\\d"", replace = "" "", text)"
"0","#Lowercase words"
"0","text3 <- tolower(text2)"
"0","#remove single words "
"0","text4 <- gsub(pattern = ""\\b[A-z]\\b{1}"", replace = "" "", text3) "
"0","#Remove whitespace"
"0","text5 <- stripWhitespace(text4)"
"0","#Lematize terms in its dictionary form"
"0","text6 <- lemmatize_strings(text5, dictionary = lexicon::hash_lemmas)"
"0",""
"0","adicional_stopwords <- c(""good"",""represent"", ""present"", ""different"",""london"", ""may"",""datum"",""taipei"", ""numb"", ""much"", ""one"", ""two"", ""can"", ""fig"", ""will"", ""arm"", ""along"", ""xpj"", ""figure"", ""thus"",""aviv"", ""tel"", ""dsc"",""dscs"",""traf"",""also"",""study"", stopwords(""en""))"
"0","#remove stopwords"
"0","text7 <- removeWords(text6, adicional_stopwords)"
"0",""
"0","# Remove words for bigrams"
"0","new_stopwords <- c(""ow"", ""exible"", ""cantly"",""wick"", ""exibility"", ""uence"", ""uences"", ""ned"")"
"0","text_bigram <- removeWords(text7, new_stopwords)"
